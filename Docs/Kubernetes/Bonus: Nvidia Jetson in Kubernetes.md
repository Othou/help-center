<p>In this guide's bonus section, we'll examine the process of incorporating an Nvidia Jetson module into your K3s cluster, thereby adding a machine learning node to the cluster. We'll label the node, then deploy a test deployment to confirm that it has access to the GPU on the node.</p>
<h1>Setup</h1>
<p>To configure your Nvidia Jetson as a worker node in Kubernetes, it is important to note that there are some unique requirements to be aware of. Firstly, <span class="wysiwyg-color-red"><strong>you must use Nvidia's JetPack OS</strong></span> for the installation process and cannot use DietPi. For the best outcome, it is recommended to follow the official Nvidia documentation for installing JetPack OS on your specific Jetson module. Once you have the module set up within Turing Pi V2 and can access it through SSH, you can move forward with the setup process. If you need additional support, you can refer to our guide for installing the OS, which is available <a href="https://help.turingpi.com/hc/en-us/articles/8942913487901" target="_blank" rel="noopener"><strong>HERE</strong></a>. It is also important to make sure that you have <span class="wysiwyg-color-red"><strong>installed the NVIDIA Container Runtime</strong></span> from the Jetson SDK components and any other relevant tools.</p>
<p>Â </p>
<p><img src="https://help.turingpi.com/hc/article_attachments/9103214470557" alt="nvidia.png"></p>
<p>To verify, you can check it by logging in using the following command:</p>
<pre>root@ubuntu:~# nvidia-container-runtime --version<br>runc version 1.1.0-0ubuntu1~18.04.1<br>spec: 1.0.2-dev<br>go: go1.16.2<br>libseccomp: 2.5.1</pre>
<p>Upon logging in, you will find that JetPack is based on Ubuntu Linux. To verify the version of Ubuntu you have installed, switch to the root user and run a command to check.</p>
<p>Please note that this is on our Jetson Nano</p>
<pre>root@ubuntu:~# lsb_release -a<br>No LSB modules are available.<br>Distributor ID: Ubuntu<br>Description: Ubuntu 18.04.6 LTS<br>Release: 18.04<br>Codename: bionic</pre>
<p>First do the update:</p>
<pre>apt-get update<br>apt-get upgrade<br>apt autoremove</pre>
<p>Next, you will need to install the following packages:</p>
<pre>apt-get -y install python3-pip curl<br>pip3 install -U jetson-stats</pre>
<p>Disable IPv6 by adding these lines to the bottom of the file: <code>/etc/sysctl.conf</code></p>
<pre># Disable IPv6<br>net.ipv6.conf.all.disable_ipv6 = 1<br>net.ipv6.conf.default.disable_ipv6 = 1<br>net.ipv6.conf.lo.disable_ipv6 = 1</pre>
<p>To ensure that your node has a fixed IP address, you will need to set it using your network-specific values. If necessary, you can adjust these values to suit your needs.</p>
<p>To make this change, you will need to <strong>edit</strong> the file "<strong>/etc/default/networking</strong>" and include the following line:</p>
<pre>CONFIGURE_INTERFACES=no</pre>
<p>Once you have made the change to "/etc/default/networking", you will need to <strong>edit</strong> the file "<strong>/etc/network/interfaces</strong>" and add your network configuration details.</p>
<pre>auto eth0<br>iface eth0 inet static<br>  address 10.0.0.63<br>  netmask 255.255.255.0<br>  gateway 10.0.0.254</pre>
<p>Make sure package nvidia-jetpack is installed (It should be, by default)</p>
<pre>apt-cache show nvidia-jetpack</pre>
<p>You will also need to <strong>edit</strong> the "<strong>/etc/hosts</strong>" file to make sure that it is consistent across your entire Kubernetes environment. In this case, as we are using the Kubernetes environment from our previous guide, we will name the Jetson module "cube04". It is important to ensure that the "/etc/hosts" file is the same on all nodes in your Kubernetes environment.</p>
<pre>127.0.0.1 localhost<br>10.0.0.60 cube01 cube01.local<br>10.0.0.61 cube02 cube02.local<br>10.0.0.62 cube03 cube03.local<br>10.0.0.63 cube04 cube04.local</pre>
<p>Additionally, you will need to set the new hostname using a command-line interface.</p>
<pre>root@ubuntu:/etc/network# hostnamectl set-hostname cube04<br>root@ubuntu:/etc/network# hostnamectl status<br>Static hostname: cube04<br>Icon name: computer<br>Machine ID: a3d9197b765643568af09eb2bd3e5ce7<br>Boot ID: 4c6eb1bc3b004a228e3f97d227ff5cdd<br>Operating System: Ubuntu 18.04.6 LTS<br>Kernel: Linux 4.9.299-tegra<br>Architecture: arm64<br><br></pre>
<h2><span class="wysiwyg-color-red"><strong>Bug</strong></span></h2>
<p>As of the writing of this guide on February 3, 2023, there is a <a href="https://github.com/NVIDIA/k8s-device-plugin/issues/377" target="_blank" rel="noopener"><strong>bug</strong> </a>in versions of the nvidia-container-toolkit prior to 1.12 that prevent it from functioning properly on Jetson within a Kubernetes environment. To determine the version you have installed, you can run the following command:</p>
<pre>dpkg -l nvidia-container-toolkit</pre>
<p><strong>If the version you have installed is less than 1.12, you will need to switch to the experimental branch and update to a newer version. This will ensure that your nvidia-container-toolkit will work correctly with your Jetson in a Kubernetes environment.</strong></p>
<pre>distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \<br>&amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \<br>&amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/experimental/$distribution/libnvidia-container.list | \<br>sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \<br>sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</pre>
<pre>apt update<br>apt upgrade<br>reboot<br># just to be sure, try again<br>apt upgrade </pre>
<p><strong>Log back in on the new fixed IP we set up.</strong></p>
<p>You can now use <strong>jtop</strong> command to check your Jetson module:</p>
<p><img src="https://help.turingpi.com/hc/article_attachments/9102553640605" alt="jtop_1.png"></p>
<p><img src="https://help.turingpi.com/hc/article_attachments/9106467591325" alt="jtop_2.png"></p>
<p><strong>It's important to note that installing the full SDK and JetPack can consume a significant amount of storage space. Keep this in mind as you plan your installation process.</strong></p>
<pre>root@cube04:~# df -h<br>Filesystem Size Used Avail Use% Mounted on<br>/dev/mmcblk0p1 14G 13G 461M 97%  /</pre>
<p>We can do some clean up.</p>
<pre>apt autoremove -y<br>apt clean<br># Remove office and thunderbird<br>apt remove thunderbird libreoffice-* -y<br># Remove sample files<br>rm -rf /usr/local/cuda/samples<br>rm -rf /usr/src/cudnn_samples_*<br>rm -rf /usr/src/tensorrt/data<br>rm -rf /usr/src/tensorrt/samples<br>rm -rf /usr/share/visionworks* ~/VisionWorks-SFM*Samples<br>rm -rf /opt/nvidia/deepstream/deepstream*/samples<br># Remove GUI<br>apt-get purge gnome-shell ubuntu-wallpapers-bionic light-themes chromium-browser* libvisionworks libvisionworks-sfm-dev -y<br>apt-get autoremove -y<br>apt clean -y<br># Remove static libraries <br>rm -rf /usr/local/cuda/targets/aarch64-linux/lib/<em>.a</em><br>rm -rf <em>/usr/lib/aarch64-linux-gnu/libcudnn</em>.a<br>rm -rf /usr/lib/aarch64-linux-gnu/libnvcaffe_parser*.a<br>rm -rf /usr/lib/aarch64-linux-gnu/libnvinfer*.a<br>rm -rf /usr/lib/aarch64-linux-gnu/libnvonnxparser*.a<br>rm -rf /usr/lib/aarch64-linux-gnu/libnvparsers*.a</pre>
<p>This should clean it up a little.</p>
<pre>root@cube04:~# df -h<br>Filesystem Size Used Avail Use% Mounted on<br>/dev/mmcblk0p1 14G 9.2G 3.9G 71% /</pre>
<h2>Joining Kubernetes</h2>
<p>This process should work similarly to the process of joining other worker nodes in your Kubernetes environment.</p>
<pre># Install k3s and join to master<br>curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.60:6443 K3S_TOKEN=myrandompassword sh -<br># On Master node, label the new node<br>kubectl label nodes cube04 kubernetes.io/role=worker<br>kubectl label nodes cube04 node-type=jetson</pre>
<p>K3s should detect the nvidia-container-runtime on its own. You can verify this by checking the file "/var/lib/rancher/k3s/agent/etc/containerd/config.toml". If everything is set up correctly, you should see lines similar to the following</p>
<pre>[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]<br>runtime_type = "io.containerd.runc.v2"<br>[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]<br>BinaryName = "/usr/bin/nvidia-container-runtime</pre>
<p>It's crucial to make the NVIDIA runtime environment the default one to avoid potential issues. To do this, perform the following necessary steps on the Jetson node.</p>
<pre>cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl</pre>
<p>Edit the <strong>/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl </strong>and under <strong>plugins."io.containerd.grpc.v1.cri".containerd </strong>add <strong>default_runtime_name = "nvidia"</strong></p>
<pre>.<br>.<br>.<br>[plugins."io.containerd.grpc.v1.cri".containerd]<br>  snapshotter = "overlayfs"<br>  disable_snapshot_annotations = true<br>  default_runtime_name = "nvidia"<br>.<br>.<br>.</pre>
<pre># Restart containerd and k3s service<br>systemctl restart containerd<br>systemctl restart k3s-agent</pre>
<h2>Not enough space ?</h2>
<p>It's possible that your Jetson node may not have sufficient storage space to run containers, especially if you're using the 16 GB Jetson Nano. In this case, you may want to consider using an M.2 SSD and mounting it to "/containerd". Additionally, you should create the following folders: "/containerd/run-k3s", "/containerd/var-k3s-pods", and "/containerd/var-k3s-rancher". These folders will be linked to the original locations in containerd. Unfortunately, the "--state" location is hard-coded to "k3s", so you'll need to use the "ln -s" command to link the folders.</p>
<pre># Locate your M.2<br>lsblk | grep disk<br># For us, it's this one: nvme0n1 259:0 0 465.8G 0 disk<br># Create ext4 partition<br>mkfs.ext4 /dev/nvme0n1<br># Create directory where we will mount the disk <br>mkdir /containerd<br># Add following to /etc/fstab <br>/dev/nvme0n1 /containerd ext4 defaults 0 1<br># Mount the disk<br>mount -a<br># Check <br>df -h /containerd<br># Create two folders<br>mkdir -p /containerd/run-k3s<br>mkdir -p /containerd/var-k3s-pods<br>mkdir -p /containerd/var-k3s-rancher<br># Stop containerd and k3s service<br>systemctl stop k3s-agent<br>systemctl stop containerd<br>/usr/local/bin/k3s-killall.sh<br># Move files to new location<br>mv /run/k3s/ /containerd/run-k3s/<br>mv /var/lib/kubelet/pods/ /containerd/var-k3s-pods/<br>mv /var/lib/rancher/ /containerd/var-k3s-rancher/<br># Create symbolic links<br>ln -s /containerd/run-k3s/ /run/k3s<br>ln -s /containerd/var-k3s-pods/ /var/lib/kubelet/pods<br>ln -s /containerd/var-k3s-rancher/ /var/lib/rancher<br># Start K3s agent<br>systemctl start k3s-agent</pre>
<h1>Tests</h1>
<p><strong>First docker, on your Nvidia Jetson node run:</strong></p>
<pre>docker run --rm --runtime nvidia xift/jetson_devicequery:r32.5.0</pre>
<p>If everything is ok, you should get detected CUDA device like this:</p>
<pre>./deviceQuery Starting...<br><br>CUDA Device Query (Runtime API) version (CUDART static linking)<br><br>Detected 1 CUDA Capable device(s)<br><br>Device 0: "NVIDIA Tegra X1"<br>CUDA Driver Version / Runtime Version            10.2 / 10.2<br>CUDA Capability Major/Minor version number:      5.3<br>Total amount of global memory:                   3963 MBytes (4155203584 bytes)<br>( 1) Multiprocessors, (128) CUDA Cores/MP:       128 CUDA Cores<br>GPU Max Clock rate:                              922 MHz (0.92 GHz)<br>Memory Clock rate:                               13 Mhz<br>Memory Bus Width:                                64-bit<br>L2 Cache Size:                                   262144 bytes<br>Maximum Texture Dimension Size (x,y,z)           1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)<br>Maximum Layered 1D Texture Size, (num) layers    1D=(16384), 2048 layers<br>Maximum Layered 2D Texture Size, (num) layers    2D=(16384, 16384), 2048 layers<br>Total amount of constant memory:                 65536 bytes<br>Total amount of shared memory per block:         49152 bytes<br>Total number of registers available per block:   32768<br>Warp size:                                       32<br>Maximum number of threads per multiprocessor:    2048<br>Maximum number of threads per block:             1024<br>Max dimension size of a thread block (x,y,z):    (1024, 1024, 64)<br>Max dimension size of a grid size (x,y,z):       (2147483647, 65535, 65535)<br>Maximum memory pitch:                            2147483647 bytes<br>Texture alignment:                               512 bytes<br>Concurrent copy and kernel execution:            Yes with 1 copy engine(s)<br>Run time limit on kernels:                       Yes<br>Integrated GPU sharing Host Memory:              Yes<br>Support host page-locked memory mapping:         Yes<br>Alignment requirement for Surfaces:              Yes<br>Device has ECC support:                          Disabled<br>Device supports Unified Addressing (UVA):        Yes<br>Device supports Compute Preemption:              No<br>Supports Cooperative Kernel Launch:              No<br>Supports MultiDevice Co-op Kernel Launch:        No<br>Device PCI Domain ID / Bus ID / location ID: 0 / 0 / 0<br>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.2, NumDevs = 1<br>Result = PASS</pre>
<p>Next, we do the same with <strong>containerd:</strong></p>
<pre>ctr i pull docker.io/xift/jetson_devicequery:r32.5.0<br>ctr run --rm --gpus 0 --tty docker.io/xift/jetson_devicequery:r32.5.0 deviceQuery</pre>
<p>You should get the same result as from Docker image.</p>
<h1>Nvidia Device Plugin</h1>
<p>We can now install <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="noopener">Nvidia Device Plugin,</a> from their git:</p>
<p dir="auto">The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to automatically:</p>
<ul dir="auto">
<li>Expose the number of GPUs on each nodes of your cluster</li>
<li>Keep track of the health of your GPUs</li>
<li>Run GPU enabled containers in your Kubernetes cluster.</li>
</ul>
<p>This means that the GPU availability on each node is detected, and we can use special toleration and resource requests for containers. This helps to utilize the GPU effectively. However, it is not mandatory, as we can also use nodeSelector to select the Jetson node specifically.</p>
<h2>Install</h2>
<p>Always check the<a href="https://github.com/NVIDIA/k8s-device-plugin/releases" target="_blank" rel="noopener"> git</a> for latest version. <strong>Do this on your Master control node</strong>.</p>
<pre>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.13.0/nvidia-device-plugin.yml</pre>
<p>Wait a while, and then you can check if all the DaemonSets are running (they should be all running, even on nodes without GPU)</p>
<pre>root@cube01:~# kubectl get pods -n kube-system | grep nvidia-device-plugin<br>nvidia-device-plugin-daemonset-gpljh 1/1 Running 0 50m<br>nvidia-device-plugin-daemonset-2hsmv 1/1 Running 0 50m<br>nvidia-device-plugin-daemonset-4b2dj 1/1 Running 0 50m<br>nvidia-device-plugin-daemonset-cn7zf 1/1 Running 2 (28m ago) 50m</pre>
<p>You can check the logs of each, on the node where the GPU is you should see:</p>
<pre>root@cube01:~# kubectl logs nvidia-device-plugin-daemonset-cn7zf -n kube-system<br>2023/02/03 12:08:05 Starting FS watcher.<br>2023/02/03 12:08:05 Starting OS watcher.<br>2023/02/03 12:08:05 Starting Plugins.<br>2023/02/03 12:08:05 Loading configuration.<br>2023/02/03 12:08:05 Updating config with default resource matching patterns.<br>2023/02/03 12:08:05 <br>Running with config:<br>{<br>"version": "v1",<br>"flags": {<br>"migStrategy": "none",<br>"failOnInitError": false,<br>"nvidiaDriverRoot": "/",<br>"gdsEnabled": false,<br>"mofedEnabled": false,<br>"plugin": {<br>"passDeviceSpecs": false,<br>"deviceListStrategy": "envvar",<br>"deviceIDStrategy": "uuid"<br>}<br>},<br>"resources": {<br>"gpus": [<br>{<br>"pattern": "*",<br>"name": "nvidia.com/gpu"<br>}<br>]<br>},<br>"sharing": {<br>"timeSlicing": {}<br>}<br>}<br>2023/02/03 12:08:05 Retreiving plugins.<br>2023/02/03 12:08:05 Detected non-NVML platform: could not load NVML: libnvidia-ml.so.1: cannot open shared object file: No such file or directory<br>2023/02/03 12:08:05 Detected Tegra platform: /etc/nv_tegra_release found<br>2023/02/03 12:08:05 Starting GRPC server for 'nvidia.com/gpu'<br>2023/02/03 12:08:05 Starting to serve 'nvidia.com/gpu' on /var/lib/kubelet/device-plugins/nvidia-gpu.sock<br>2023/02/03 12:08:05 Registered device plugin for 'nvidia.com/gpu' with Kubelet</pre>
<p>Because Nvidia Jetson does not use fully featured GPU, we have some not found items, but what is important is: <strong>Registered device plugin for 'nvidia.com/gpu' with Kubelet.</strong></p>
<p>You can now check your node for this GPU resource:</p>
<pre>kubectl describe node cube04<br>.<br>.<br>Capacity:<br>cpu: 4<br>ephemeral-storage: 14384136Ki<br>hugepages-2Mi: 0<br>memory: 4057816Ki<br>nvidia.com/gpu: 1<br>pods: 110<br>Allocatable:<br>cpu: 4<br>ephemeral-storage: 13992887490<br>hugepages-2Mi: 0<br>memory: 4057816Ki<br>nvidia.com/gpu: 1<br>pods: 110<br>.<br>.<br>.</pre>
<p>As you can see, we have one nvidia.com/gpu: 1 available.</p>
<h2>Deploy test pod</h2>
<p>Create file <strong>tensor.yaml</strong></p>
<pre>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: nvidia-query<br>spec:<br>  restartPolicy: OnFailure<br>  nodeSelector:<br>    node-type: jetson<br>  containers:<br>  - name: nvidia-query<br>    image: xift/jetson_devicequery:r32.5.0<br>    command: [ "./deviceQuery" ]</pre>
<p>This one option, we have used here:</p>
<pre>nodeSelector:<br>  node-type: jetson</pre>
<p>We have labeled our node Jetson near the start of this guide.</p>
<p>The other option is to use the GPU annotation from NVIDIA plugin.</p>
<pre>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: nvidia-query<br>spec:<br>  restartPolicy: OnFailure<br>  containers:<br>  - name: nvidia-query<br>    image: xift/jetson_devicequery:r32.5.0<br>    command: [ "./deviceQuery" ]<br>    resources:<br>      limits:<br>        nvidia.com/gpu: 1 # requesting 1 GPU<br>  tolerations:<br>  - key: nvidia.com/gpu<br>    operator: Exists<br>    effect: NoSchedule<br><br></pre>
<p>Apply to cluster:</p>
<pre>kubectl create -f tensor.yaml<br># Check<br>root@cube01:~# kubectl get pods<br>NAME READY STATUS RESTARTS AGE<br>nvidia-query 0/1 Completed 0 87s<br># Check the log<br>root@cube01:~# kubectl logs nvidia-query<br>.<br>.<br>.<br>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.2, NumDevs = 1<br>Result = PASS</pre>
<p>That's it! Your Kubernetes cluster is now up and running and able to automatically detect both CPU and GPU resources. It can assign and manage GPU resources for pods as needed.</p>